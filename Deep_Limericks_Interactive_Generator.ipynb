{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/DeepLimericks/blob/main/Deep_Limericks_Interactive_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Limericks**\n",
        "**I once trained an AI to rhyme, It took GPT-J a long time,</br>\n",
        "Since the Colab was slow, I upgraded to Pro,</br> Each limerick cost me a dime.**\n",
        "\n",
        "By Robert. A Gonsalves</br>\n",
        "\n",
        "![image](https://raw.githubusercontent.com/robgon-art/DeepLimericks/main/deep_limericks_med.jpg)\n",
        "\n",
        "You can see my article on Medium.\n",
        "\n",
        "The source code and generated Haikus are released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/).</br>\n",
        "![CC BYC-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)\n"
      ],
      "metadata": {
        "id": "r-XpbHy4_RkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initiialize System\n",
        "\n",
        "!nvidia-smi -L\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = torch.clone(DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias))\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J\n",
        "\n",
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "!gdown 1-slQ8AZIPzzIiF9XUhEGwiuantLQ1caz\n",
        "\n",
        "gpt = torch.load(\"/content/deep_limericks.pt\",  map_location=torch.device('cuda'))\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yHW45ab76mIg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter a topic for your limerick\n",
        "#@markdown Leave blank to use randomly generated topics.\n",
        "topic = 'kitten' #@param {type:\"string\"}\n",
        "has_topic = len(topic) > 0\n",
        "if has_topic:\n",
        "  print(\"topic:\", topic)\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt1 = \"<\" + topic + \" =T2R=\"\n",
        "  result_length = 30\n",
        "  inputs = tokenizer(prompt1, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "  \n",
        "  rhyme_sets = []\n",
        "  topics = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    rhyme_set = text[text.find(\" =T2R= \")+len(\" =T2R= \"):text.rfind(\">\")]\n",
        "    parts = rhyme_set.split(\" \\ \")\n",
        "    rhymes = []\n",
        "    for p in parts:\n",
        "      rhyme = p.strip()\n",
        "      if len(rhyme) > 0 and rhyme not in rhymes:\n",
        "        rhymes.append(rhyme)\n",
        "    if rhyme_set not in rhyme_sets and len(rhymes) == 5:\n",
        "      rhyme_sets.append(rhyme_set)\n",
        "      if not has_topic:\n",
        "        topics.append(rhymes[0])\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# rhyme_sets.sort()\n",
        "\n",
        "print(\"rhyme sets:\\n\" + 85  * '-')\n",
        "for i, r in enumerate(rhyme_sets):\n",
        "  d = r.replace(\"\\\\\", \"/\")\n",
        "  if has_topic:\n",
        "    print(str(i+1) +\":\", d)\n",
        "  else:\n",
        "    print(str(i+1) +\":\", \"topic:\", topics[i]+\":\", d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "m-P9I2HV78xV",
        "outputId": "c4202c6b-d4ac-4a97-d363-3b98745a48c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "\n",
            "rhyme sets:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: kitty / pretty / buns / runs / pity\n",
            "2: kitten / dissension / cravings / wares / pension\n",
            "3: kitty / pretty / meek / peek / witty\n",
            "4: pet / get / cat / that / bet\n",
            "5: mitten / kitten / puss / fuss / dinner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a rhyme-set\n",
        "\n",
        "import re\n",
        "rhyme_set_selection = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "rss = min(rhyme_set_selection, len(rhyme_sets))\n",
        "rhyme_text = rhyme_sets[rss-1]\n",
        "\n",
        "if not has_topic:\n",
        "  topic = topics[rss-1]\n",
        "print(\"topic:\", topic)\n",
        "print(\"rhyme-set:\", rhyme_text.replace(\"\\\\\", \"/\"))\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt2 = \"<\" + topic + \": \" + rhyme_text + \" =R2L= \"\n",
        "  result_length = 40\n",
        "  inputs = tokenizer(prompt2, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "\n",
        "  first_rhyme = rhyme_text.split(\" \\ \")[0].strip().lower()\n",
        "  first_lines = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    text = ' '.join(text.split())\n",
        "    trimmed_text = text[text.find(\" =R2L= \")+len(\" =R2L= \"):text.rfind(\">\")]\n",
        "    lines = trimmed_text.split(\" / \")\n",
        "    if len(lines) >=  1  and len(lines[0]) > 0:\n",
        "      first_line = lines[0]\n",
        "      last_word = first_line.split()[-1].strip().lower()\n",
        "      last_word = re.sub(r'[^a-z]+', '', last_word)\n",
        "      if first_rhyme == last_word and first_line not in first_lines:\n",
        "        first_lines.append(first_line)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "first_lines.sort()\n",
        "print(\"first lines:\\n\" + 85  * '-')\n",
        "for i, f in enumerate(first_lines):\n",
        "  print(str(i+1) +\":\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7cQ10N-oBZXW",
        "outputId": "d52b2dbd-8fef-4613-9d0f-b28ec0f1bc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "rhyme-set: kitty / pretty / meek / peek / witty\n",
            "\n",
            "first lines:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: \"You're a beautiful kitten, my kitty,\n",
            "2: A kitten is named, or a kitty\n",
            "3: A kitten's a cute little kitty,\n",
            "4: My cat's a petite little kitty,\n",
            "5: My kitten's a feline named Kitty\n",
            "6: When I'm watching TV with my kitty,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose the first line\n",
        "first_line_selection = 4 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "fls = min(first_line_selection, len(first_lines))\n",
        "first_line = first_lines[fls-1]\n",
        "\n",
        "print(\"topic:\", topic)\n",
        "print(\"rhyme-set:\", rhyme_text.replace(\"\\\\\", \"/\"))\n",
        "print()\n",
        "print(\"first_line:\", first_line)\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt3 = \"<\" + topic + \": \" + rhyme_text + \" =R2L= \" + first_line + \" / \"\n",
        "  result_length = 80\n",
        "  inputs = tokenizer(prompt3, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "\n",
        "  second_rhyme = rhyme_text.split(\" \\ \")[1].strip().lower()\n",
        "  second_lines = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    text = ' '.join(text.split())\n",
        "    trimmed_text = text[text.find(\" =R2L= \")+len(\" =R2L= \"):text.rfind(\">\")]\n",
        "    lines = trimmed_text.split(\" / \")\n",
        "    if len(lines) >= 2 and len(lines[1]) > 0:\n",
        "      second_line = lines[1]\n",
        "      last_word = second_line.split()[-1].strip().lower()\n",
        "      last_word = re.sub(r'[^a-z]+', '', last_word)\n",
        "      if second_rhyme == last_word and second_line not in second_lines:\n",
        "        second_lines.append(second_line)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "second_lines.sort()\n",
        "print(\"second lines:\\n\" + 85  * '-')\n",
        "for i, f in enumerate(second_lines):\n",
        "  print(str(i+1) +\":\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "lqQepYJTDSfR",
        "outputId": "f3a8c962-b34f-4619-b992-18cb26456d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "rhyme-set: kitty / pretty / meek / peek / witty\n",
            "\n",
            "first_line: My cat's a petite little kitty,\n",
            "\n",
            "second lines:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: And I know she's not fat at all, pretty.\n",
            "2: And her coat is all white and so pretty.\n",
            "3: And her eyes are like diamonds. She's pretty.\n",
            "4: So she's all of her name, and she's pretty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose the second line\n",
        "\n",
        "import re\n",
        "second_line_selection = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "sls = min(second_line_selection, len(second_lines))\n",
        "second_line = second_lines[sls-1]\n",
        "\n",
        "print(\"topic:\", topic)\n",
        "print(\"rhyme-set:\", rhyme_text.replace(\"\\\\\", \"/\"))\n",
        "print()\n",
        "print(\"first_line: \", first_line)\n",
        "print(\"second_line:\", second_line)\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt4 = \"<\" + topic + \": \" + rhyme_text + \" =R2L= \" + first_line \n",
        "  prompt4 += \" / \" + second_line + \" / \"\n",
        "  result_length = 60\n",
        "  inputs = tokenizer(prompt4, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "\n",
        "  third_rhyme = rhyme_text.split(\" \\ \")[2].strip().lower()\n",
        "  third_lines = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    text = ' '.join(text.split())\n",
        "    trimmed_text = text[text.find(\" =R2L= \")+len(\" =R2L= \"):text.rfind(\">\")]\n",
        "    lines = trimmed_text.split(\" / \")\n",
        "    if len(lines) >= 3 and len(lines[2]) > 0:\n",
        "      third_line = lines[2]\n",
        "      last_word = third_line.split()[-1].strip().lower()\n",
        "      last_word = re.sub(r'[^a-z]+', '', last_word)\n",
        "      if third_rhyme == last_word and third_line not in third_lines:\n",
        "        third_lines.append(third_line)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "third_lines.sort()\n",
        "print(\"third lines:\\n\" + 85  * '-')\n",
        "for i, f in enumerate(third_lines):\n",
        "  print(str(i+1) +\":\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "jEZSGLbPG0Yp",
        "outputId": "f2ceed6d-549a-4ebd-9f74-4512cd21c5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "rhyme-set: kitty / pretty / meek / peek / witty\n",
            "\n",
            "first_line:  My cat's a petite little kitty,\n",
            "second_line: And her eyes are like diamonds. She's pretty.\n",
            "\n",
            "third lines:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: But she never makes meek,\n",
            "2: I have named her, \"Meek\n",
            "3: I'm not sure if she's meek\n",
            "4: She is not very meek\n",
            "5: She's a bit shy and meek,\n",
            "6: She's a kitten and meek,\n",
            "7: She's a sweet little meek \n",
            "8: She's a sweet, loving meek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose the third line\n",
        "\n",
        "import re\n",
        "third_line_selection = 4 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "tls = min(third_line_selection, len(third_lines))\n",
        "third_line = third_lines[tls-1]\n",
        "\n",
        "print(\"topic:\", topic)\n",
        "print(\"rhyme-set:\", rhyme_text.replace(\"\\\\\", \"/\"))\n",
        "print()\n",
        "print(\"first_line :\", first_line)\n",
        "print(\"second_line:\", second_line)\n",
        "print(\"third_line :\", third_line)\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt5 = \"<\" + topic + \": \" + rhyme_text + \" =R2L= \" + first_line \n",
        "  prompt5 += \" / \" + second_line + \" / \" + third_line + \" / \"\n",
        "  result_length = 70\n",
        "  inputs = tokenizer(prompt5, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "\n",
        "  fourth_rhyme = rhyme_text.split(\" \\ \")[3].strip().lower()\n",
        "  fourth_lines = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    text = ' '.join(text.split())\n",
        "    trimmed_text = text[text.find(\" =R2L= \")+len(\" =R2L= \"):text.rfind(\">\")]\n",
        "    lines = trimmed_text.split(\" / \")\n",
        "    if len(lines) >= 4 and len(lines[3]) > 0:\n",
        "      fourth_line = lines[3]\n",
        "      last_word = fourth_line.split()[-1].strip().lower()\n",
        "      last_word = re.sub(r'[^a-z]+', '', last_word)\n",
        "      if fourth_rhyme == last_word and fourth_line not in fourth_lines:\n",
        "        fourth_lines.append(fourth_line)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "fourth_lines.sort()\n",
        "print(\"fourth lines:\\n\" + 85  * '-')\n",
        "for i, f in enumerate(fourth_lines):\n",
        "  print(str(i+1) +\":\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "f4IPym24HdPp",
        "outputId": "43c388a8-29e3-4dfb-a6ff-53de54bdae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "rhyme-set: kitty / pretty / meek / peek / witty\n",
            "\n",
            "first_line : My cat's a petite little kitty,\n",
            "second_line: And her eyes are like diamonds. She's pretty.\n",
            "third_line : She is not very meek\n",
            "\n",
            "fourth lines:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: And does not like to peek\n",
            "2: And she's got a nice peek\n",
            "3: But she likes to take a peek\n",
            "4: When she wants to peek\n",
            "5: When she's eating her peek.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose the fourth line\n",
        "\n",
        "import re\n",
        "fourth_line_selection = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "fls = min(fourth_line_selection, len(fourth_lines))\n",
        "fourth_line = fourth_lines[fls-1]\n",
        "print(\"topic:\", topic)\n",
        "print(\"rhyme-set:\", rhyme_text.replace(\"\\\\\", \"/\"))\n",
        "print()\n",
        "print(\"first_line  :\", first_line)\n",
        "print(\"second_line :\", second_line)\n",
        "print(\"third_line  :\", third_line)\n",
        "print(\"fourth_line :\", fourth_line)\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt5 = \"<\" + topic + \": \" + rhyme_text + \" =R2L= \" + first_line \n",
        "  prompt5 += \" / \" + second_line + \" / \" + third_line + \" / \"\n",
        "  prompt5 += fourth_line + \" / \"\n",
        "  result_length = 150\n",
        "  inputs = tokenizer(prompt5, return_tensors=\"pt\").to('cuda:0')\n",
        "  beam_outputs = gpt.generate(inputs[\"input_ids\"],\n",
        "    max_length=result_length,\n",
        "    top_k=50, top_p=0.95, \n",
        "    do_sample=True, temperature=0.7, pad_token_id=50256,\n",
        "    num_return_sequences=10)\n",
        "\n",
        "  fifth_rhyme = rhyme_text.split(\" \\ \")[4].strip().lower()\n",
        "  fifth_lines = []\n",
        "\n",
        "  for beam_output in beam_outputs:\n",
        "    text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "    text = ' '.join(text.split())\n",
        "    trimmed_text = text[text.find(\" =R2L= \")+len(\" =R2L= \"):text.rfind(\">\")]\n",
        "    lines = trimmed_text.split(\" / \")\n",
        "    if len(lines) >= 5 and len(lines[4]) > 0:\n",
        "      fifth_line = lines[4]\n",
        "      last_word = fifth_line.split()[-1].strip().lower()\n",
        "      last_word = re.sub(r'[^a-z]+', '', last_word)\n",
        "      if fifth_rhyme == last_word and fifth_line not in fifth_lines:\n",
        "        fifth_lines.append(fifth_line)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "fifth_lines.sort()\n",
        "print(\"fifth lines:\\n\" + 85  * '-')\n",
        "for i, f in enumerate(fifth_lines):\n",
        "  print(str(i+1) +\":\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "kI5kQsOIIRBQ",
        "outputId": "89505e37-677b-4f8b-e761-ecec90c0695a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic: kitten\n",
            "rhyme-set: kitty / pretty / meek / peek / witty\n",
            "\n",
            "first_line  : My cat's a petite little kitty,\n",
            "second_line : And her eyes are like diamonds. She's pretty.\n",
            "third_line  : She is not very meek\n",
            "fourth_line : And does not like to peek\n",
            "\n",
            "fifth lines:\n",
            "-------------------------------------------------------------------------------------\n",
            "1: At the cat burglar who's clever and witty.\n",
            "2: Out of windows. She's not very witty.\n",
            "3: at my finger when I'm writing witty.\n",
            "4: at the camera. She's not very witty.\n",
            "5: at the mirror. She's witty.\n",
            "6: in your pants—she's not witty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose final line\n",
        "\n",
        "import re\n",
        "fifth_line_selection = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "vls = min(fifth_line_selection, len(fifth_lines))\n",
        "fifth_line = fifth_lines[vls-1]\n",
        "print(\"Final Limerick\")\n",
        "print()\n",
        "print(first_line)\n",
        "print(second_line)\n",
        "print(third_line)\n",
        "print(fourth_line)\n",
        "print(fifth_line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "iNMUsvCTJI1S",
        "outputId": "673ad3ab-0e37-40f7-86cb-24e939168d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Limerick\n",
            "\n",
            "My cat's a petite little kitty,\n",
            "And her eyes are like diamonds. She's pretty.\n",
            "She is not very meek\n",
            "And does not like to peek\n",
            "At the cat burglar who's clever and witty.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}